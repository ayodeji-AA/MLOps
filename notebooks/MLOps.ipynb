{"cells":[{"cell_type":"code","source":["from mlflow.recipes import Recipe\nimport pyspark.sql.functions as F"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7bc58fef-0dee-42e0-bd61-b0a48de48848","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def ingest_data(file_path):\n    \"\"\"\n    Ingest data from different file formats and determine the format\n    \"\"\"\n    \n    if file_path.endswith(\".csv\"):\n        df = spark.read.csv(file_path, header=True, inferSchema=True)\n        print(\"The data is in CSV format.\")\n    elif file_path.endswith(\".parquet\"):\n        df = spark.read.parquet(file_path)\n        print(\"The data is in Parquet format.\")\n    elif file_path.endswith(\".json\"):\n        df = spark.read.json(file_path)\n        print(\"The data is in JSON format.\")\n    else:\n        print(\"The format of the data is not supported.\")\n        return\n    \n    return df\n    \n    raise NotImplementedError\n\n\ndef count_missings(spark_df):\n    \"\"\"\n    Counts number of nulls and nans in each column\n    \"\"\"\n    df = spark_df.select([F.count(F.when(F.isnan(c) |\\\n                                         F.col(c).isNull() |\\\n                                         (F.col(c) == '') |\\\n                                         F.col(c). contains ('None'), c)).alias(c) for c in spark_df.columns])\n\n    col_counts = [F.sum(F.when(F.col(col) != 0, 1).otherwise(0)).alias(col) for col in df.columns]\n    result = df.agg(*col_counts).collect()[0].asDict()\n    for col, count in result.items():\n        if count != 0:\n            print(f\"Column '{col}' has null values\")\n            df.filter(F.col(col).isNull()).show()\n    if all(val == 0 for val in result.values()):\n        print(\"There are no null values\")\n\n\n\ndef check_duplicates(df):\n    \"\"\"\n    Check for duplicate rows in a Spark dataframe\n    \"\"\"\n    count = df.count()\n    distinct_count = df.distinct().count()\n    if count == distinct_count:\n        print(\"There are no duplicate rows in the dataframe.\")\n    else:\n        duplicates = df.groupBy().agg(F.count(\"*\").alias(\"count\")) \\\n            .filter(F.col(\"count\") > 1)\n        print(\"Duplicate rows detected in the following columns:\")\n        duplicates.show()\n        \ndef replace_spaces(df):\n    \"\"\"\n    Check for spaces in columns and replace them with underscores\n    \"\"\"\n    has_space = False\n    for col_name in df.columns:\n        if \" \" in col_name:\n            has_space = True\n            df = df.withColumnRenamed(col_name, col_name.replace(\" \", \"_\"))\n\n    if has_space:\n        print(\"Spaces in column names have been replaced with underscores.\")\n    else:\n        print(\"There are no spaces in column names.\")\n        \n    return df\n\n\ndef detect_data_type(df):\n    \"\"\"\n    Detects the data type of each column in a PySpark dataframe\n    \"\"\"\n    # Get the schema of the dataframe\n    schema = df.schema\n\n    # Iterate over the schema to get the data type of each column\n    for field in schema:\n        print(\"Column: {}, Data Type: {}\".format(field.name, field.dataType))\n\n\n### path should be '/user/hive/warehouse/{project name}'\ndef create_delta_table(df, database, table, path):\n    \"\"\"\n    Creates a Delta Lake database and reads a DataFrame into the database\n    \"\"\"\n    \n    # Write the DataFrame to a Delta Lake database\n    df.write.format(\"delta\").mode(\"overwrite\").save(path)\n    spark.sql(f'create database if not exists {database}')\n    \n    spark.sql(\"\"\"\n      CREATE TABLE IF NOT EXISTS {}.{}\n      USING DELTA\n      LOCATION '{}'\n    \"\"\".format(database,table,path)\n    )\n    \n    return None\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5c55c3ab-e8b2-4dbf-954e-a5e7937e91cb","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["###Ingest Dataset"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b359b0b6-55f2-48df-8452-9ab244bb5734","inputWidgets":{},"title":""}}},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0911063c-d74d-43ca-877d-bd2aa5514df5","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"MLOps","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":362840237713054}},"nbformat":4,"nbformat_minor":0}
